\documentclass[../Main.tex]{subfiles}
\usepackage{tabularx}

\begin{document}
\chapter{Data Analytics}

\intro{
    Data Analytics is the process of examining, cleaning, transforming, and 
    modelling data to extract useful insights, support decision-making, and 
    identify patterns. 
    It involves cleaning, transforming, and modelling data using statistical 
    analysis, machine learning, and data visualization to interpret complex 
    datasets. 
    Businesses, researchers, and organisations use data analytics to optimise 
    performance, predict trends, and drive strategic decisions.
}

\defn{Datenkompetenz (Data Literacy)}{
Umfasst die Fähigkeiten, Daten auf kritische Art und Weise zu sammeln, zu 
managen, zu bewerten und anzuwenden.

Grundlegende Fragen:
    \begin{enumerate}
        \item Was will ich mit Daten machen?
        \item Was kann ich mit Daten machen?
        \item Was darf ich mit Daten machen?
        \item Was soll ich mit Daten machn?
    \end{enumerate}
}

\section{Datentypen und Skalenniveaus}
Die Bestimmung der Datenmerkmale ermöglicht es erst, Daten zu 
organisieren, zu analysieren und schliesslich zu nutzen.
Das führt zu Skalenniveaus (Attributs-Merkmals-Levels, en: "Levels of 
measurement" or "Scales of measure")

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datentypen.png}
    \caption{Datentypen}
\end{figure}


\begin{multicols}{2}
    \defn{Qualitative Daten}{
        \begin{itemize}
            \item Beziehen sich auf Informationen die nicht gemessen werden
            \item In der Regel beschreibend also in Textform (können auch numerisch codiert sein)
        \end{itemize}
    }
    \defn{Quantitative Daten}{
        \begin{itemize}
            \item Werden verwendet, um Informationen zu definieren, die gezählt werden können
            \item Sind numerisch
        \end{itemize}
    }
\end{multicols}

\defn{NOIR}{
    \begin{itemize}
        \item Nominalskala
        \item Ordinalskala
        \item Intervallskala
        \item Verhältnisskala (Ratio)
    \end{itemize}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/noir.png}
    \caption{NOIR}
\end{figure}

\newpage

\section{Business Intelligence (BI)}
Datenvisualisierungs- und –Publikation-Werkzeuge ist allgemeiner  
und auch kurz Business Intelligence-Werkzeug (BI Tools) genannt.
Früher sprach man auch von Decision Support Systems.

\begin{multicols}{2}
    \defn{BI Tools}{
        Sind Tools und Technologien, die Unternehmen einsetzen, um Daten zu sammeln, zu 
        analysieren und in aussagekräftige Erkenntnisse und Wissen umzuwandeln, das für 
        fundierte Geschäftsentscheidungen genutzt werden kann. Umfasst den Einsatz von 
        Analyse, Data Mining, Data Warehousing und Reporting.
    }
    \defn{Decision Support Systems (DSS)}{
        Sind computergestützte Informationssysteme mit Methoden, die Entscheidungsträgern 
        helfen, bessere und fundiertere Entscheidungen zu treffen (bezüglich Ist-Zustand, 
        zurückblickenden Analyse).

        Dabei gibt es verschiedene Ansätze:
        \begin{itemize}
            \item Datenbankansatz (inkl. Visualisierung)
            \item Data-Mining-Ansatz
            \item Informationsbeschaffungsansatz
        \end{itemize}
    }
    BI liefert die Daten und Erkenntnisse, die für die Entscheidungsfindung erforderlich sind, 
    während DSS die Werkzeuge und Methoden für diese Entscheidungen bereitstellt.
\end{multicols}

Die wohl verbreitetsten Produkte sind die kommerziellen Anwendungen:
\begin{itemize}
    \item Tableau
    \item Ms Power BI
\end{itemize}
Weitere Produkte:
\begin{itemize}
    \item Looker
    \item Apache Superset
    \item Oracle Analytics
    \item Datawrapper
\end{itemize}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/evolutionbi.png}
    \caption{Evolution of BI}
\end{figure}

\defn{7 Types of Quantitative Messages}{
    \begin{itemize}
        \item Nominal Comparison
        \item Time-Series
        \item Part-to-Whole
        \item Deviation
        \item Frequency distribution
        \item Correlation
    \end{itemize}
    Stephen Few
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\linewidth]{Images/quantitative-messages.png}
    \caption{Quantitative Messages}
\end{figure}

\defn{Graphical Integrity by Tufte}{
    \begin{enumerate}
        \item The representation of numbers, as physically 
        measured on the surface of the graph itself, 
        should be directly proportional to the 
        numerical quantities represented.
        \item Clear, detailed and thorough labeling should 
        be used to defeat graphical distortion and 
        ambiguity. Write out explanations of the data 
        on the graph itself. Label important events in 
        the data.
        \item Show data variation, not design variation.
        \item In time-series displays of money, 
        deflated and standardized units of 
        monetary measurement are nearly 
        always better than nominal units.
        ("Deflating" means adjusting for inflation,
        so the values reflect purchasing power rather
        than just raw monetary amounts.)
        \item  The number of information carrying 
        (variable) dimensions depicted 
        should not exceed the number of 
        dimensions in the data.
        Graphics must not quote data out of context.
    \end{enumerate}
}

\defn{Data Ink Principles by Tufte}{
    \begin{enumerate}
        \item Above all else show data.
        \item Maximize the data-ink ratio.
        \item Erase non-data-ink.
        \item Erase redundant data-ink.
        \item Revise and edit
    \end{enumerate}
    The data-ink ratio is calculated by 1 minus the proportion of the graph 
    that can be erased without loss of data-information.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/visualization-guideline.png}
    \caption{Visualization Guideline by Abela}
\end{figure}

\href{https://en.wikipedia.org/wiki/Visual_variable}{Visual Variable (Wikipedia)}


\section{OLAP}

\defn{Online Transaction Processing (OLTP)}{
    Zum Beispiel Flugbuchungssysteme.
    Charakterisierung:
    \begin{itemize}
        \item Hohe paralellität
        \item Viele kurze Transaktionen
        \item Transaktionen bearbeiten kleines Datenvolumen
        \item Mission-Critical
        \item Hohe Verfügbarkeit
        \item Normalisierte Relationen
        \item Wenige Indexe
    \end{itemize}
}

\defn{OLAP}{
    Überbegriff für Technologien, Methoden und Tools zur Ad-hoc-Analyse 
    multidimensionaler Informationen.
    Die Daten werden nach unterschiedlichsten (ad-hoc) Kriterien 
    ausgewertet.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/oltp-olap.png}
    \caption{OLTP vs OLAP}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/olap-übersicht.png}
    \caption{OLAP Übersicht}
\end{figure}

\defn{Data Warehouse}{
    Ein Data Warehouse ist ein Datenbanksystem (für OLAP
    Anwendungen), in dem die entscheidungsrelevanten Daten eines 
    Unternehmens in konsolidierter Form gesammelt werden, um sie für 
    (zeitbasierte) Auswertungen zugänglich zu machen.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/data-warehouse.png}
    \caption{Datawarehouse: Orchestriert Daten für weitere Analysen}
\end{figure}

\defn{ETL - Extract-Transform-Load}{
    \begin{itemize}
        \item Aufbereitung von Daten aus heterogenen Quellen
        \item Laden ins Data-Warehouse
    \end{itemize}
}

\section{Relationale Data Warehouses}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/sternschema.png}
    \caption{Sternschema: In diesem Schema werden verschiedene Dimensionstabellen eienr Faktentabelle zugeordnet}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/sternschema-beispiel.png}
    \caption{Beispiel einer Umsetzung des Sternschemas. Die Faktentabelle setzt sich ausschliesslich aus Foreign-Keys zusammen.}
\end{figure}

\defn{Sternschema}{
    Die Faktentabelle ist normalisiert und typischerweise sehr gross bis mehrere Mio Tuples,
    sie beinhaltet Bewegungsdaten mit weiteren Attributen, die Referenzen/FKs auf Dimensionstabellen sind.
    Die Dimensionstabellen beschreiben die Fakten in der Faktentabelle und sind
    oft nicht normalisiert und klein bis mittelgross (etwa 1'000).
    In der Regel werden Queries über 1-stufige Joins zwischen Fakten und verknüpften Dimensionen getätigt.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/schneflockenschema.png}
    \caption{Die Normalisierung der Dimensionstabellen führt zum Schneeflocken-Schema.}
\end{figure}

\defn{Schneeflocken-Schema}{
    Normalisierung der Dimensionen im Sternschema führt zum Schneeflocken-Schema.
    Pro Hierarchielevel kann in der Regel mit einer Tabelle gerechnet werden.
    Das bedeutet Abfragen über Joins werden kompliziert, daher hat sich das Schema eher nicht durchgesetzt.
}

\subsection{Data-Warehouse Design Grundsätze}
auch "Dimensional Modeling" genannt.
\begin{itemize}
    \item Faktentabellen enthalten nur numerische Werte und Fremdschlüssel
    \item Faktentabellen sollten so granular wie möglich aufgebaut sein. (einfacher zum Aggregieren)
    \item Dimensionstabellen haben in der Regel einen Surrogat-Schlüssel
    \item Granularität der Zeit-Dimension kann nicht kleiner sein, als die Auflösung der Zeit im operativen System
    \item "Conformed Dimensions" haben für alle möglichen Fact-Tables eine einheitliche Bedeutung
    \item Hierarchien in Dimensionen sind möglichst als funktionale Abhängigkeit zwischen den Attributen einer
    flachen Dimensionstabelle oder als Fremdschlüsselbeziehung zwischen Dimensionstabellen zu konzipieren.
\end{itemize}

\subsection{Fakt-Tabellen Klassifizierung}
\begin{description}
    \item[Transaction Fact Table] Ereignis, dass zu einem bestimmten Zeitpunkt geschah
    \item[Periodic Snapshot] kumulative Grösse einer Kennzahl gemessen in regelmässigen Intervallen
    \item[Accumulating Snapshot] Eintrag beschreibt eine Business-Transaktion aus mehreren Schritten zu unterschiedlichen Zeitpunkten   
\end{description}

\subsection{Spezielle Dimensionen}
\begin{description}
    \item[Degenerate Dimension] Identifizieren eine Transaktion (z.B Order Header)
    \item[Role Playing Dimension] Dimension wird von Fact-Table mehrfach unterschiedlich bedeutend referenziert
    \item[Junk Dimensions] Korrelierte Indikationen und Flags 
\end{description}

\defn{Slowly Changing Dimensions}{
    "Veränderungen treten unerwartet, sporadisch und weitaus seltener auf als Messungen in Faktentabellen"
    \begin{description}
        \item[Typ 1] Keine Historisierung (Overwrite)
        \item[Typ 2] Historisierung mittels neuem Zeilen-Eintrag und Attribute PK-BK valid-from und valid-to
        \item[Typ 3 bis 6] Weitere Historisierung durch zusätzliche Zeit-Attribute oder Tabellen  
    \end{description}
}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/scd-typ-2.png}
    \caption{SCD Typ 2}
\end{figure}

Typ 2 SCD wird z.B von MSSQL und Azure SQL unterstütz unter dem Namen "system-versioned temporal tables".

\section{Aggregationsfunktionen und Pivot}

\defn{GROUP BY ROLLUP}{
    Ähnlich zu GROUPBY mit unterschiedlichen Detailierungsgraden,
    Bei ROLLUP(A,B) werden Daten zuerst nach A und dann nach B aggregiert und anschliessen noch ein Total beigefügt.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/rollup.png}
    \caption{ROLLUP Befehl}
\end{figure}

\defn{GROUP BY CUBE}{
    Ähnlich wie der ROLLUP-Befehl fügt jedoch jedes Zwischentotal dazu.
    Erfordert für die Generierung die Potenzmenge der zu aggregierenden Spalten.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/cube.png}
    \caption{CUBE Befehl}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/groupingset.png}
    \caption{Grouping Sets Operator; Dieser kann generell anstelle von ROLLUP und CUBE eingesetzt werden
    Zwischenresultate müssen explizit verlangt werden.}
\end{figure}

\defn{Pivot-Tabelle}{
    Tabellenspalten werden aggregiert und transponiert, dies nennt man Pivotieren (drehen).
    Es wird typischerweise in Statistik und Marktforschung verwendet zusammen mit Aggregationsfunktionen.
    Pivot- und Kreuz-Tabellen sind praktisch synonyme.

    Bei PSQL nicht integriert aber kann statisch via "crosstab" oder dynamisch über "tablefunc" erstellt werden.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/pivot.png}
    \caption{Pivot}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/datamarts.png}
    \caption{Data Marts: Teilmenge eines grösseren DW}
\end{figure}

\defn{Data Marts}{
    Für den Zugriff auf Teilmenge eines Data Warehouses für bestimmte Subeinheiten eines Systems.
    Data Marts sind kleiner und flexibler als das Datawarehouse.
    Sie können strukturierte oder unstrukturierte Daten schnell und durch optimierte Abfragen (Vorberechnung \& Speicherung)
    bereitstellen. (z.B Materialized View, Cube-Datastructures)

    Wenn weitere Optimierung nötig ist, kann z.B MDX von Microsoft benutzt werden, welche Data-Cubes als
    Datenstrukturen und Abfragesprache zur Verfügung stellt.
}

\defn{Data Lake}{
    Speichersystem für strukturierte und unstrukturierte Daten in ihrem ursprünglich en Format.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.72\linewidth]{Images/da-begriffe.png}
\end{figure}

\subsection{Cube}
Ein Cube besteht aus Measures und Dimensions. Die Measure beschreiben numerische Grössen mit denselben Dimensionsverknüpfungen.
Dimensionen beschreiben Measure-Groups. Eine Dimension besteht aus einem oder mehereren Attributen.
Jedes Attribut kann eine Hierachie aufweisen. Auf den Dimensionen können Hierarchien zwischen den Attributen definiert werden.
Abfragen können über spezialisierte Spracvhen getätigt werden z.B. Microsoft MDX.

\section{Spatial Data}
\defn{Geo-Informationssystem (GIS) mit EVAP}{
    
Geografisches Informationssystem zur:
    \begin{itemize}
        \item Erfassung
            \begin{itemize}
                \item GPS (Outdoor)
                \item DGPS (Outdoor)
                \item 3D-Scanner (Kinect)
                \item WLAN
                \item Bluetooth
                \item Kamera (SLAM)
            \end{itemize}
        \item Verwaltung (Speicherung)
            \begin{itemize}
                \item Kommerziell: ArcGIS, Oracle, MSSQL, MongoDB
                \item Open Source: PostGreSQL, MySQL, DuckDB
                \item Geodatentypen, Funktionen, Indexe
            \end{itemize}
        \item Analyse
        \item Präsentation Darstellung
        \end{itemize}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/evap-eingabe.png}
    \caption{EVAP: Eingabe / Erfassung}
\end{figure}

\defn{Geodaten (Geospatial Data)}{
    Sind allgemein Daten mit Raumbezug
    \begin{itemize}
        \item Geografische Namen
        \item Adressen
        \item Codes
        \item Punkte, Linien, Flächen, Pixel/Voxel
    \end{itemize}
    Geodaten weisen eine komplexe Datenstruktur mit vielen Formaten auf.
    Sie sind:
    \begin{itemize}
        \item komplexe Konsistenzbedingungen
        \item Grosser Erfassungsaufwand
        \item Grosse Datenmengen
        \item Viele Metadaten
            \begin{itemize}
                \item Koordinaten Referenzsystem
                \item Auflösung
                \item Qualität
            \end{itemize}
    \end{itemize}
}

\subsection{Raumbezug - Projektion}
Eine 2D Abbildung unseres 3D Raumes ist schwierig.
Der Modellierungsprozess erfordert abstraktion, welcher zu Informationsverlust führen kann.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/raumbezug.png}
    \caption{Raumbezug - Projektion}
\end{figure}
Längengrad/Breitengrad sind ein "geozentrischer 
Vektor". Karten und Bildschirme sind jedoch 
"eben/flach".
\\\\
Eine Projektion bildet das Ellipsoid auf eine Fläche 
ab, d.h. auf kartesische Koordinaten: der Nordwert 
(Mathe Y) und der Ostwert (Mathe X) und stehen 
rechtwinklig zueinander.
\\\\
Koordinatensysteme inkl. Projektionen erhalten 
eine Abkürzung, z.B. CHLV95 (Schweizerische 
Landesvermessung 1995) oder WGS84 (World 
Geodetic System) und einen Code

\defn{Bekannte Codes}{
    \begin{itemize}
        \item WGS84 - EPSG-Code:4326 ("GPS-Koordinaten")
            \begin{itemize}
                \item geozentrisch, geografisch, Grad
                \item Beiten/Längengrad (lat/lon N/E)
            \end{itemize}
        \item Web Mercator - EPSG-Code 3857 ("Schulwandkarte")
            \begin{itemize}
                \item kartesisch (=eben), metrisch
            \end{itemize}
        \item CHLV95 - EPSG-Code 2056 ("Schweizer Koordinatensystem")
            \begin{itemize}
                \item Landvermessung CH neu, kartesisch-metrisch
                \item 7-stellige Koordinaten
            \end{itemize}
    \end{itemize}
    Die National Geospatial-Intelligence Agency der USA empfiehlt WGS 84 (EPSG:4326). Sie rät von 
    Web Mercator (EPSG:3857) für militärische und nachrichtendienstliche Zwecke ab, da es 
    Probleme mit der Positionsgenauigkeit gibt, insbesondere in hohen Breitengraden.
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/coordinate_precision.png}
    \caption{Koordinatenpräzision}
\end{figure}

\subsection{GIS Architecture}
\begin{itemize}
    \item  Produktpalette mit ArcGIS Pro und ArcGIS Online als bekannteste Produkte
    \item  QGIS Projekt (Open Source GPL)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/gis-architektur.png}
    \caption{GIS: Generelle Architektur}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/gis-architekturen.png}
    \caption{GIS: Produkt Architektur}
\end{figure}

\subsubsection{Web-Apps}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/gis-webapps.png}
    \caption{GIS: Webapps}
\end{figure}

\subsubsection{Geovisualisierung für Data Engineers}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/geovisualisierung-data-eng.png}
    \caption{Geovisualisierung für Data Engineers}
\end{figure}

\subsection{Prinzipien von GIS Verarbeitung}
\defn{Ebenenprinzip}{
    Informationen werden in Ebenen übereinander geschichtet.
    Koordination ermöglichen einen vertikalen Lagenvergleich.
    Die Stärke von GIS ist durch Kombination unabhängiger Daten neue Informationen abzuleiten.
}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/gis-layers.png}
    \caption{Layerprinzip}
\end{figure}

\defn{Universelle Fremdschlüssel (Raumbezug)}{
    Der Raumbezug wird als gemeinsamen Nenner aller Objekte herangezogen.
    Koordinaten sttelllen einen grundsätzlich neuen Beziehungstyp dar,
    der Kombinationen z.b via räumlichen Joins erlaubt.
}

\defn{Modellierung / Repräsentation}{
    \begin{itemize}
        \item Diskret (Räumlich begrenzt), z.B Gebäude
        \item Kontinuierlich (Felder), z.B Luftdruck
    \end{itemize}
    Diese werden codiert:
    \begin{itemize}
        \item Vektorbasiert (Vektordaten)
            \begin{itemize}
                \item Geometrische Datentypen Punktkoordinaten, Linien und Flächen
                \item Einzeln ansprechbar, gruppierbar, skalierbar, "berechenbarer"
            \end{itemize}
        \item Rasterbasiert (Rasterdaten, z.B Bilder)
            \begin{itemize}
                \item zusammengehörige Menge von (Farb-)Pixeln, die einen Raumbezug haben (sie 
                sind sogenannt "geo-referenziert")
                \item Erfasst durch CCD/CMOS-Sensoren in Scanner und Kameras; z.B. GeoTIFF-Format
                \item Schneller in Darstellung als Vektoren, billiger in der Herstellung
            \end{itemize}
    \end{itemize}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/geodaten-zu-grafik.png}
    \caption{Geodaten zu Grafik}
\end{figure}

\subsubsection{Geodaten Vektor Formate}
\begin{itemize}
    \item GeoJSON
        \begin{itemize}
            \item Lesbar
        \end{itemize}
    \item GeoPackage
        \begin{itemize}
            \item Verbessertes Shapefile
            \item Erweiterung SQLite
            \item Binärformat
        \end{itemize}
    \item Shapefile
        \begin{itemize}
            \item Industrie Norm
            \item Binär
            \item 2 Abhängige Dateien (Geometrie (.shp) + Sachdaten (.dbf))
            \item Veraltet
        \end{itemize}
    \item DXF
        \begin{itemize}
            \item Reine Geometrie/CAD
            \item Keine Sachdaten
        \end{itemize}
    \item GeoParquet
        \begin{itemize}
            \item Erweiterung Parquet
            \item Spaltenorientiert
            \item Offenes Speicherformat für effiziente Komprimierung (\&schnelle Abfragen)
        \end{itemize}
    \item CSV
    \item SVG
\end{itemize}
Konvertierungsprogramme:
\begin{itemize}
    \item https://geoconverter.infs.ch/
    \item https://mygeodata.cloud/converter/
\end{itemize}

\subsubsection{Geodaten Raster Formate}
\begin{itemize}
    \item PDF
    \item PNG, JPG
    \item TIFF, GeoTIFF
        \item Standard
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/postgis-vektordatentypen.png}
    \caption{PostGIS Vektordatentypen}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/geonormen.png}
    \caption{Geo-Normen}
\end{figure}

\begin{lstlisting}[language=SQL]
    CREATE TABLE eisenbahn (
        id int4 PRIMARY KEY, 
        name TEXT,
        geom GEOMETRY('LINESTRING', 2056)
    );

    CREATE INDEX 
        in_eisenbahnen_the_geom
        ON eisenbahnen
        USING gist(geom);


    INSERT INTO uster(id, name, geom) VALUES (
        100,
        'Uster'
        ST_GeometryFromText('LINESTRING(8.81 47.22,8.81 47.22)',4326)  
    );

    SELECT id, name, ST_AsText(geom) FROM uster;

    CREATE TABLE beers (
        id GENERATED ALWAYS AS IDENTITY,
        name VARCHAR(60),
        price NUMERIC(10, 2),
        geom GEOMETRY('POINT', 4326)
        -- use usually a planar CRS not a geocentric
    );

    select name, price,
        round((price + 0.001 * ST_DistanceSphere(geom, 
            ST_GeometryFromText(
                'POINT(8.816511 47.223064)',4326)))::numeric, 2
            ) as net_price
    from beers order by net_price;
\end{lstlisting}

\subsection{PostGIS Queries}
\begin{lstlisting}[language=SQL]
-- Aufgabe 5.1: Distanzberechnung
-- => Im Lokalkoordinatensystem
SELECT ST_distance(
    (SELECT geom FROM orte 
        WHERE "name" = 'Bern' AND type = 'Medium City'),
    (SELECT geom FROM orte
        WHERE "name" = 'Zurich'AND type = 'Medium City'));
-- => basierend auf einer Kugel
SELECT ST_distanceSphere(
    (SELECT ST_Transform(geom,4326) FROM orte
        WHERE "name" = 'Bern' AND type = 'Medium City'),
    (SELECT ST_Transform(geom,4326) FROM orte
        WHERE "name" = 'Zurich'AND type = 'Medium City')
    );
-- => basierend auf einem Bessel 1841 Ellipsoid.
SELECT ST_distanceSpheroid(
    (SELECT ST_Transform(geom,4326) FROM orte
        WHERE "name" = 'Bern' AND type = 'Medium City'),
    (SELECT ST_Transform(geom,4326) FROM orte
        WHERE "name" = 'Zurich'AND type = 'Medium City'),
    'SPHEROID["Bessel 1841",6377397.155,299.1528128]');


-- Aufgabe 5.2: Objektselektion mit angrenzenden Flaechen
-- Selektieren Sie alle Gemeinden,
-- die an die Gemeinde Rapperswil-Jona grenzen.
SELECT * FROM gemeinden
WHERE ST_Touches(
    geom, 
    (SELECT geom FROM gemeinden WHERE name = 'Rapperswil-Jona')
) 
ORDER BY name ASC;


-- Aufgabe 5.3: Objektselektion als Umkreissuche
-- Selektieren Sie alle Orte im Umkreis
-- von 10 km um die HSR (CH1903: 704472/231216)
SELECT name, geom 
FROM orte
WHERE ST_DWithin(geom, 
    ST_GeomFromText('POINT(704472 231216)', 21781),10000)
ORDER BY name ASC;


-- Aufgabe 5.4: Pufferzone
-- Selektieren Sie alle Orte, die in einer Pufferzone
-- von 2km um den Fluss Emme liegen.
SELECT name 
FROM orte
WHERE ST_Within(
    geom,
    (SELECT ST_Buffer(geom, 2000) FROM fluesse WHERE name = 'Emme')
)
ORDER BY name ASC;


-- Aufgabe 5.5. 
-- Schreiben Sie eine Query, die alle Gemeinden selektiert,
-- durch die der Fluss Emme fliesst. 
SELECT g.name 
FROM gemeinden g
WHERE ST_Intersects(geom, (SELECT geom FROM fluesse WHERE name = 'Emme'))
ORDER BY 1;
-- oder 
SELECT g.name 
FROM gemeinden g
JOIN fluesse f ON ST_Intersects(f.geom, g.geom)
WHERE f.name = 'Emme' 
ORDER BY 1;
    
\end{lstlisting}

\section{Big Data}
\defn{Big Data}{
    Zu "Big Data" gib es keine einheitliche Definition; wird häufig auch als 
    Sammelbegriff verwendet für (Real-Time-)Analyse massiver Datenmengen.

    Unterteilunge auch möglich:
    \begin{itemize}
        \item Small Data: Memory eines einzelnen Systemes
        \item Medium Data: Auf Disk eine Systems
        \item Big Data: Erfordert verteilte Systeme
    \end{itemize}

    Blogbeitrag von Jordan Tigani (*), Zusammenfassung:
    \begin{itemize}
        \item Meisten Daten <100GB
        \item Wenn doch, wahrscheinlich nicht alle auf einmal abgefragt
    \end{itemize}
    (*) "Big Data is Dead" (2023) by Jordan Tigani, DuckDB Maintainer (Open Source); 
    vorher bei Google BigQuery: https://motherduck.com/blog/big-data-is-dead/ 
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/4v-bigdata.png}
    \caption{Die 4 V's von Big Data}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/treiber-bigdata.png}
    \caption{Die Treiber von Big Data}
\end{figure}

\defn{Big Data Analytics}{
    Vielmals ist bei Big Data Analytics real-time analysen erforderlich.
    Dazu sind traditionelle Data Warehouse Architekuren ungeeignet.
    \begin{description}
        \item[Ansatz 1] Scaling Up, In-Memory Column Stores
        \item[Ansatz 2] Scaling Out, Shared Nothing Architektur, Verteilte Systeme  
    \end{description}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/bigdata-tech.png}
    \caption{Big Data Technology Landscape}
\end{figure}

\section{Analytical Processing}
\defn{In-Memory Column Store}{
    \begin{itemize}
        \item \textbf{Column Store:} Spaltenorientiertes DMBS
            \begin{itemize}
                \item Bessere Komprimierung
                \item Bessere Indexierung
                \item Datenverwaltung pro Spalte
            \end{itemize}
        \item \textbf{In-Memory:}
            \begin{itemize}
                \item Nur Memory
                \item Keine Persistenz
                \item Persistenz durch logischem WAL oder Snapshots
            \end{itemize}
    \end{itemize}
    Bietet aufgrund der Architektur schnellere OLAP-Lese-Anfragen (10-100x)
    sind jedoch langsamer bei Updates und daher ungeeignet für write-heavy OLTP.
}

Produkte:
\begin{itemize}
    \item Oracle 12 (Erweiterung)
    \item MSSQL (Erweiterung)
    \item PostgreSQL (Third Party Erweiterung)
    \item SAP Hana (Standard, teuer, grosse Ressourcenanforderungen (Memory 500GB))
    \item Hbase
\end{itemize}

Weitere Optimierungen:
\begin{itemize}
    \item Materialisierte Strukturen
    \item Indexe
    \item Parallel Query Processing
    \item Vectorized Query Processing Style (Tupel Batches als Vektoren, SIMD) vs. Volcano (einzelne Tupel)
\end{itemize}

\section{Verteilte Technologien und Tools}
\subsection{Map-Reduce}
MapReduce war lange ein Kernkonzept in der Big-Data-Welt, insbesondere in Hadoop-Umgebungen.
Es ist noch in bestimmten Szenarien wichtig, wie z.B. Massive Batch-Jobs, Log
Analysen, verteilte Berechnungen ohne Echtzeitanforderungen.
Technologien wie Apache Spark, Dask, Vaex und Cloud-Data-Warehouses (z.B. 
BigQuery, Snowflake) sind inzwischen populärer.
\begin{itemize}
    \item Google
    \item Funktionales Modell gür gross angelegte Datenverarbeitung mittels vielen Computern
    \item Hohe Parallelität und Verfügbarkeit
    \item HDFS (Hadoop File System)
    \item Map-Reduce wird immer auf Disk geschrieben (langsamer als Spark)
    \item Hohe Abstraktion für den Benutzer (einfachere Handhabung)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/map-reduce.png}
    \caption{Map-Reduce Diagramm}
\end{figure}

Nachteile:
\begin{itemize}
    \item Master ist SPOF
    \item Master ist Engpass für Skalierbarkeit
    \item Latenz beim Öffnen von Dateien
\end{itemize}

\subsection{Spark}
Ist eine Open-Source (2010) Big-Data-Framework mit verteilter Architektur.
Benutzt schnelles In-Memory Computing.
Geschrieben in Scala mit Spark API für Python, Java, R und Clojure (Lisp)

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/spark-architektur.png}
    \caption{Spark Architektur: Driver ist z.B der Benutzer via PySpark.
    Spark erstellt Abstraktionsebene auf Datenablage. 
    Beliebige Nodes können hinzugefügt werden (Horizontale Skalierung).}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/RDD.png}
    \caption{RDD's sind immutable und können nur als Input oder Output von Funktionen verwendet werden}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/spark-ecosystem.png}
    \caption{Spark Ecosystem}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/spark-operations.png}
    \caption{Spark Operations}
\end{figure}

Leistungsmerkmale:
\begin{itemize}
    \item Kann Daten vollkommen In-Memory verarbeiten (im Vergleich zu Map-Reduce)
    \item Wenn RDD nicht in Memory passt, verschlechtert sich die Performance
\end{itemize}

\subsection{Spark Notebook}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/spark-driver.png}
    \caption{Spark Driver JVM}
\end{figure}
Jeder Executor hat eine Anzahl "Slots" (Apache Spark nennt diese "Cores", auch wenn sie nicht direkt mit 
den physischen CPU-Kernen zu tun haben). Das sind Threads, auf denen die eigentliche Rechenarbeit geschieht.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/spark-driver-slot.png}
    \caption{Der "Driver" schickt Aufgaben (Tasks) an die leeren Slots der Executors, um Arbeit aufzuteilen}
\end{figure}

\begin{lstlisting}[language=Python]
# DF API is similar to pandas API
fire_service_calls_df = spark.read.csv(
        SF_FIRE_CALL_CSV_URL, header=True, schema=fire_schema
    )
# Column Names
fire_service_calls_df.columns

# Row Count
fire_service_calls_df.count()
# -> 6 892 945
\end{lstlisting}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/spark-df-actions.png}
    \caption{Spark Aktionen auf oder an DF}
\end{figure}
Im Gegensatz zu Transformationen auf Pandas-DataFrames wird beim Aufruf
von Spark-DataFrame-Transformationen noch nichts "wirklich" ausgeführt,
sondern nur die Transformation im Query-Plan des zurückgegebenen neuen DataFrames vermerkt.

Erst beim Aufruf einer Aktion wird der Query-Plan (mit allen so angesammelten Transformationen und mit der Aktion)
optimiert und dann ausgeführt.

\begin{lstlisting}[language=Python]
# select distinct call types
fire_service_calls_df.select('CallType').distinct().show()


# count distinct call types
fire_service_calls_df.select('CallType').distinct().count()
\end{lstlisting}

\begin{lstlisting}[language=Python]
# register new sql context on df for sql queries
sqlContext.registerDataFrameAsTable(
        fire_service_calls_df, "fireServiceCalls"
    )

# query via sql
# %sql ist eine Databricks-spezifische "Notebook Magic", 
# die in Jupyter (oder in Python-Programmen, die Spark als 
# Bibliothek verwenden) nicht zur Verfuegung steht.
%sql
SELECT CallType
FROM fire_service_calls_df
LIMIT 5;

# Oder via Python
spark.sql(
  """
  SELECT CallType
  FROM fireServiceCalls
  LIMIT 5
  """
).show()
\end{lstlisting}


\begin{lstlisting}[language=Python]
# schema ausgeben
fire_service_calls_df.printSchema()

from pyspark.sql.functions import year
# Wieviele Jahre umfassen die vorliegenden Notrufdaten?
fire_service_calls_ts_df
    .select(year('CallDateTS'))
    .distinct()
    .orderBy('year(CallDateTs)')
    .show()

# Bis zu welchem Datum gehen die Daten?
fire_service_calls_ts_df
    .agg({"CallDateTS": "max"})
    .collect()

# Anruf Zahl pro Tag
count_df = fire_service_calls_ts_df \
  .filter('CallDateTS >= "2015-06-28"') \
  .filter('CallDateTS <= "2015-07-10"') \
  .groupBy('CallDateTS') \
  .count() \
  .orderBy('CallDateTS')

count_df.show()

# Gibt die Anzahl Paritionen zurueck
# in welche die Daten verteilt werden
fire_service_calls_df.rdd.getNumPartitions()

# Neupartitionierung
fire_service_calls_ts_8partitions_df = 
    fire_service_calls_ts_df.repartition(8)
\end{lstlisting}

Wenn ein Job sich auf mehrere Tasks aufteilen lässt, bearbeitet jeder Task eine dieser Partitions.
Die Tasks werden auf zur Ausführung auf die Executor-Slots verteilt und die Ergebnisse dann zusammengeführt.
Bei der Ausführung können Tasks, die gleichzeitig in verschiedenen Slots liegen, parallel ausgeführt werden.

\subsubsection{Persistance}
Durch persisiertung speichert jeder Knoten alle von ihm berechneten Partitionen im SPeicher und verwendet sie in anderen Aktionen wieder.
Künftige Aktionen können somit schneller ausgeführt werde.

\begin{lstlisting}[language=Python]
# persistieren
RDD.persist() # MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, DISK_ONLY
RDD.unpersist()
\end{lstlisting}

\section{Cloud Computing}
\defn{Cloud Computing (NIST)}{
    Ein Modell für den bequemen, bedarfsorientierten Netzzugang zu einem gemeinsam genutzten Pool konfigurierbarer 
    Rechenressourcen (z.B. Netze, Server, Speicher, Anwendungen und Dienste), die mit minimalem Verwaltungs- oder 
    Interaktionsaufwand mit dem Dienstanbieter schnell bereitgestellt und freigegeben werden können.
}

\begin{table}[H]
\begin{tabularx}{\columnwidth}{X|X|X}
    \hline
    Typ & Vorteil & Nachteil \\
    \hline
    Cloud & 
    \begin{minipage}[t]{\linewidth}
        \begin{itemize}
            \item Skalierbarkeit
            \item Flexibilität
            \item geringere Initial- und Vorlaufkosten
            \item Prototyping, schneller und günstiger
            \item Für grosse Unternehmen sinnvoll, da Änderung an Architektur und Prozessen erzwingt
        \end{itemize}
    \end{minipage} & 
    \begin{minipage}[t]{\linewidth}
        \begin{itemize}
            \item Technik, Integration und Betrieb
            \item Verlust der physischen Kontrolle \& Datenschutz
            \item Teuer: Cloud-Native Prinzipien und Anpassungen
            \item Fehlende Standards
            \item Bindung an den Anbieter
            \item Unerwartete Kosten
            \item Komplexität der Verwaltung
            \item Performance-Probleme (Latenz)
        \end{itemize}
    \end{minipage} \\
    \hline
    OnPrem & \begin{itemize}
        \item Kontrolle über den Lifecycle
        \item Daten InHouse
    \end{itemize} & \begin{itemize}
        \item Meraufwand durch Lifecycle
        \item Initiale Kosten
    \end{itemize}
\end{tabularx}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/xaas.png}
    \caption{Comparison between different XaaS types}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/gartner-cloud.png}
    \caption{Cloud Magic Quadrant by Gartner}
\end{figure}

\subsubsection{Weitere Begriffe}
\begin{description}
    \item[Einfache Einrichtung und Betrieb]
    \item[Elastizität] Flexible Anpassung an die Arbeitslast.
    "Das Ausmass, in dem sich ein System an Änderung der Arbeitslast anpassen kann,
    indem es Ressourcen auf autonome Weise bereitstellt und entfernt, sodass zu jedem
    Zeitpunkt die verfügbaren Ressourcen so gut wie möglich dem aktuellen Bedarf entsprechen."
    \item[Segmentierung] Concurrency durch Partitionierung und Segmentierung maximieren.
    Verschiedene Benutzergruppen / Prozesse können isoliert auf denselben Daten arbeiten unter
    der Berücksichtigung von Priorität und Antwortzeitanforderungen.
    \item[Skalierbarkeit] Scale Up, Scale Out
    \item[Verwaltung heterogener Daten] 
\end{description}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/datan/dw-vs-dl.png}
    \caption{Data Warehouse vs Data Lake}
\end{figure}

\section{Cloud Data Analytics Services}
\subsection{Amazon Redshift}
Verfügbar seit 2012 aus AWS. Ist ein vollständig verwalteter Service der eine spalten-orientierte Datenbank bereitstellt.
\begin{itemize}
    \item Keine "on-the-fly" Elastizität
    \item Workload Manager (WLM): Segmentierung nach Priorität
    \item Begrenzte Anzahl von SQL-Funktionen zur Verwaltung semi
    strukturierter JSON-Daten
    \item Gute Integration in andere Services von AWS
    \item On-Demand günstiges Preismodell
\end{itemize}
\subsection{Snowflake}
Ist ein 2012 gegründetes, kommerzielles Produkt basierend auf einer cloud-native Architektur.
Es stellt Data Warehouse mit Data Lakes (Data Lakehouse) in form von spalten-orientierten SQL Datenbanken zur verfügung.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/datan/snowflake-1.png}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/datan/snowflake-2.png}
\end{figure}

\begin{itemize}
    \item Sofortige Elastizität
    \item Einfache Einrichtung als SaaS
    \item Gute Unterstützung von JSON Daten
    \item Jede Benutzergruppe betreibt virtuelles Warehouseauf isolierter Hardware
    \item Kostenkontrolle: Speicher und Compute getrennt
    \item Zero Copy Clone (Schnelle Kopie von Datenbanken)
\end{itemize}

\subsection{Data Lakes}
\begin{itemize}
    \item Hadoop: Kein Data Warehouse, Bietet keine Elastizität, Komplex, Initialaufwand, optimierte Speicherschichten
    \item Apache Iceberg
    \item Delta Lake: Schnellste, kleine Bekanntschaft
    \item Apache Hudi
\end{itemize}

\section{Google Earth Engine}
GEE ist Function- und Platform-as-a-Service (PaaS) mit bereits 
aufbereiteten Daten (aka Data Warehouse)
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/datan/gee-arch.png}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/gee-example.png}
\end{figure}

\begin{lstlisting}[language=Java,
                   breaklines=true,
                   breakatwhitespace=true,
                   keepspaces=true,
                   basicstyle=\ttfamily\small]
var temperatureDataset = ee.Image('NASA/ASTER_GED/AG100_003');
//https://developers.google.com/
//earth-engine/datasets/catalog/NASA_ASTER_GED_AG100_003

var nightLightsDataset = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG')
  .filterDate('2021-01-01')
  .first();
//https://developers.google.com/
//earth-engine/datasets/catalog/NOAA_VIIRS_DNB_MONTHLY_V1_VCMSLCFG?hl=en
  
var temperature = temperatureDataset.select('temperature').divide(100); // dataset is in 1/100 kelvin
var temperatureAverage = temperature.reduceNeighborhood(
  ee.Reducer.mean(),                  // calculate the mean
  ee.Kernel.square(2000, 'meters'),   // of a square with 'radius' of 2000 m (which is 4000 * 4000 m)
  "kernel",                           // using the kernel as the input weight
  true,                               // skipping masked or missing pixels in calculation
  "boxcar"                            // using the more efficient boxcar method to compute 
);
var temperatureDifferenceToSurrounding = temperature.subtract(temperatureAverage); 

var lights = nightLightsDataset.select('avg_rad').divide(60); // dataset is in range ~ 0 - 60, we want it normalized (0 - 1)
var lightsAverage = lights.reduceNeighborhood(
  ee.Reducer.mean(),
  ee.Kernel.square(4000, 'meters'), // use an area of 8000 * 8000 meters this time
  "kernel",
  true,
  "boxcar"
).multiply(10)
 .clamp(0, 1); // Multiplying and then clamping the value to 1 allows for light values as low as 0.1 to pass through the whole temperature in the next step

var wheightedTemperatureDifference = temperatureDifferenceToSurrounding.multiply(lightsAverage); // use the lights as a kind of mask, so we only see temperature in cities


// This specifies how we want our data to be displayed.
var vis = {
  min: -10, // minimum value to consider (here: minus 10 degrees kelvin difference)
  max: 10, // maximum value to consider (here: plus 10 degrees kelvin difference)
  palette: [ // color palette for displaying, ranging from blue to red 
    '0602ff', '235cb1', '307ef3', '269db1', '30c8e2', '32d3ef', '3ae237',
    'b5e22e', 'd6e21f', 'fff705', 'ffd611', 'ffb613', 'ff8b13', 'ff6e08',
    'ff500d', 'ff0000', 'de0101', 'c21301'
  ],
};

Map.setCenter(8.541111, 47.374444, 13); // Set map center and zoom

Map.addLayer(wheightedTemperatureDifference, vis, 'Temperature', true, 0.75); // Add our data as a map layer, with 75% opacity 
\end{lstlisting}

\section{ETL}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/etl-prozess.png}
\end{figure}

\begin{itemize}
    \item ETL
    \item ELT: Zuerst in das andere System übertragen dann transformieren. Einfachere Architektur und weniger Speicherplatz
    \item Zero-ETL: Direkt in ein anderes System übetragen
\end{itemize}

Datenintegration beinhaltet: Datenimport, Datenkonversion, Datenvalidierung, Datenbereinigung,
Datentransformation, Datenfusion (conflation, conciliation), Datenaggregation, Datenfilterung,
Datenberechnung, (Datenanreicherung, Datenanonymisierung, Datengenerierung) und weiteres.

\begin{enumerate}
    \item Datenbereinigung: Entfernen von Duplikaten, ungültige oder fehlende Werte
    \item Datentransformation: Schema-Abbildung, Strukturänderungen, Konvertierung von Werden, Aufteilen Spalten, Zusammenführung von Spalten
    \item Datenaggregation: Zusammenfassen von Daten aus mehreren Quellen
    \item Datenfilterung
    \item Datenberechnung
    \item Datenanreicherung: HInzufügen aus externen Quellen, Scraping, Geocoding
    \item Datenanomysierung, Generierung
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/datenintegration-dw.png}
    \caption{Schematische Darstellung des Datenflusses einer Datenintregration in ein Data Warehouse}
\end{figure}

\begin{table}[H]
    \begin{tabularx}{\columnwidth}{X|X}
        Top-Down & Ausgangspunkt vorgegebenes Zielschema (z.B. durch Standards). 
        Finden der Korrespondenzen zu lokalen Quell-Schemata und Definition der Abbildungen \\
        \hline
        Bottom-Up & Ausgangspunkt existierende Quell-Schemata.
        Konstruktion eines davon abgeleiteten, integrierten Zielschemata,
        das möglichst den vollständigen Informationsgehalt aller Quellen enthält \\
    \end{tabularx}
\end{table}
\subsection{Kriterien fpr die Integrationsmethodik}
\begin{itemize}
    \item Vollständigkeit
    \item Minimalität
    \item Verständlichkeit
    \item Korrektheit
\end{itemize}

\subsection{Typen von Integrationskonflikten}
\begin{itemize}
    \item Semantisch
    \item Beschreibung: Unterschiedliche Eigenschaften derselben Objekte; Homonyme und Synonyme; Datentypkonflikte, Wertebereiche; Masseinheiten
    \item Heterogenität: Unterschiedliche Schema; Schema-Transformation
    \item Strukturell: Unterschiedliche Modellierung; fehlende AttributeM unterschiedliche Annahmen (defaults)
    \item Datenkonflikt: Inkorrekte, veraltete Daten; Unterschiedliche codierte Werte
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/datenintegration-example.png}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/datenintegration-fehler.png}
\end{figure}

\subsection{Wartungsansätze}
\begin{itemize}
    \item Aktualisieren-Kopieren: Gesamtaustausch; Gesamtaufwand wächst mit Volumen
    \item Inkrementelle Kopie: Vorkehrungen für Anomalien aus der asynchronen Datenverarbeitung
\end{itemize}

\subsection{Pro / Contra DW in der Datenintegration}

\begin{table}[H]
    \begin{tabularx}{\columnwidth}{X|X|X}
        DW &
        Abfragen über das DW stören Quelle nicht. Können schwierige Berechnungen durchführen. Data Mining und Bereinigung &
        Zeit muss Aufgewendet werden, um das physische und logische Datenbanklayout zu entwerfen
        Daten in der Regel nicht aktuell
    \end{tabularx}
\end{table}

\subsection{Tools}
\begin{itemize}
    \item SQL: Meist fehlen Extract Tools
    \item Python mit Pandas: Mühsame Implementation und Pflege
    \item GUI: Apache Hop, OpenRefine, Open Data Editor, AirByte, CJ Workbench
    \item MS SQL Server Integration Services
    \item Azure Data Factory Studio
    \item Low Code: Apache AirFlow, Keboola
    \item Pentaho Data Integration: Kommerziell
    \item Talend Open Studio: Java, Kommerziell
    \item KNIME: ETL und ML
\end{itemize}

Nach Kategorie:
\begin{itemize}
    \item Preperation: csvkit, DuckDB, Excel, Openrefine, OKFN
    \item Datenflussmanagement: Apache Airflow, Apache NiFi
    \item Dataprocessing: Knime, Microsoft SQL Server (SSIS), IBM InfoSphere Datastage, OpenRefine
    \item Dataprocessing (Transformation): dbt, Looker Studio
    \item Hybrid: Apache Hop, FME by Safe Software, Informatica PowerCenter
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/etl-tools.png}
    \caption{Jahr 2025}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/openrefine.png}
    \caption{Opensource Desktoptool. Low Code. Spaltendarstellung. Memory basiert. Ursprünglich von Google. Verbreitet für Wikipedia}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/hop.png}
\end{figure}

\section{Data Science}
\defn{Data Science}{
    Its an interdisciplinary field:
    \begin{itemize}
        \item Business need to be undestood
        \item Data Mining for extraction
        \item Hypothesis Testing
        \item Model Evaluation
        \item Result presentation for the target audience
        \item Conclusions to be understood and fitting measures
    \end{itemize}
    The field is about extracting knowledge from data to discover new patterns and information.
}

\defn{Data Mining}{
    Refers to the extraction or collection of meaningful data.
    Looks for consistencies of patterns and relationships.
    The undelying goal is finding new knowledge.
    3 most important methods:
    \begin{itemize}
        \item Association Analysis
        \item Classification
        \item Clustering
    \end{itemize}
}

\subsection{CRISP-DM}
CRISP-DM is a industry standard defining processes for data mining.
It is a leading methodology and is divided in six phases:
\begin{enumerate}
    \item Business Understanding
    \item Data UnderstandingData Preparation
    \item Modeling
    \item Evaluation
    \item Deployment
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Images/datan/ds-q-to-methods.png}
\end{figure}

\defn{Data Objects}{
    Another taxonomy for data sample.
    A sample of a data set descibed by attributes.
}


\begin{table}[H]
    \begin{tabularx}{\columnwidth}{X|X}
        Accuracy & Wie genau sind unsere Daten? Falsche Messungen? Noise? \\
        \hline
        Completeness & Fehlende Daten? Standardwerte? \\
        \hline
        Consistency & Einheitliche Daten? Datentypen? Normalisierungsebene? \\
        \hline
        Timeliness & Aktualität, Daten frisch halten \\
        \hline
        Believability & Sind Daten glaubwürdig? \\
        \hline
        Interpretability & Verstehen wir die Daten? Domänen-Expertise \\
        \hline
        Value Adding & \\
        \hline
        Acessibility & \\
    \end{tabularx}
    \caption{Types of errors}
\end{table}

\subsection{Preprocessing}
Low quality data will lead to low quality mining results.
We use different techniques to improve data quality.
\begin{itemize}
    \item (Data) Integration
    \item Cleaning
    \item Reduction
    \item Transformation
\end{itemize}

\subsection{Basic Stat Descriptions}
\subsubsection{Mean}
\begin{figure}[H]
    \begin{equation}
        \bar{x} = \frac{\sum_{i=1}^{N} x_i}{N}
    \end{equation}
    \caption{Arithmetic Mean}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        \bar{x} = \frac{\sum_{i=1}^{N} w_i x_i}{\sum_{i=1}^{N} w_i}
    \end{equation}
    \caption{Weighted Arithmetic Mean}
\end{figure}
 Mean is sensitive to outliers this can be mitigated by using a trimmed mean.
 That is, removing the bottom and top values.
\subsubsection{Median and Mode}
The median is more robust against outliers for example in skewed data.
On an odd number of values in a set use the mean of the two middle values.
The Mode represents the most frequent value. For more information refer to stat lecture.
When multiple mode exist we refer to them by bimodal and multimodal.
\subsubsection{Dispersion of data}
\begin{figure}[H]
    \begin{equation}
        R = max(X) - min(X)
    \end{equation}
    \caption{Range; Can be wrong mislead with outliers}
\end{figure}

\begin{figure}[H]
    \begin{equation}
        q\text{-quantile} = \text{Split data in } q \text{ equal sets with } q-1 \text{ samples}
    \end{equation}
    \caption{Quantile; 4-quantiles known as quartiles and 100-quantiles as percentiles. On uneven sets calculate the middle}
\end{figure}

\begin{figure}[H]
    \begin{equation}
        k = p (n+1)
    \end{equation}
    \caption{Quartile; k = index, p = Quantilanteil (z.B. 0.25), n = Anzahl Daten}
\end{figure}

\begin{figure}[H]
    \begin{equation}
        ICR = Q_1 - Q_3
    \end{equation}
    \caption{Inter Quartile Range; Measure of spread}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.25\linewidth]{Images/datan/dispersion.png}
    \caption{Dispersion example}
\end{figure}

\defn{Five Number Summary}{
    \begin{itemize}
        \item minimum
        \item \(Q_1\)
        \item Median
        \item \(Q_3\)
        \item Maximum
    \end{itemize}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Images/datan/box-plot.png}
    \caption{Boxplots visualize a variables five number summary}
\end{figure}

\begin{figure}[H]
    \begin{equation}
        \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2
    \end{equation}
    \caption{Empiric Variance; the average of the squared differences from the mean}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        \sigma = \sqrt{\sigma^2}
    \end{equation}
    \caption{Standard Deviation; Is in the measurement of the feature}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        r_{xy} = \frac{\sum_{i=1}^{n} (x_i - \bar{x}) (y_i - \bar{y})} {\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n}(y_i - \bar{y})^2}}
    \end{equation}
    \caption{Pearson Correlation Coefficient}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        r_{xy} = \frac{\sum x_i y_i - n \bar{x} \bar{y}}{\sqrt{(\sum x_i^2 - n \bar{x}^2)} \sqrt{(\sum y_i^2 - n \bar{y}^2)}}
    \end{equation}
    \caption{Pearson Correlation Coefficient in a different form}
\end{figure}

\begin{figure}[H]
    \begin{equation}
        \chi^2 = \sum_{i=1}^{c} \sum_{j=1}^{r} \frac{ (o_{ij} - e_{ij})^2 }{ e_{ij}}
    \end{equation}
    \caption{\(\chi^2\)}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/scatter-plot.png}
    \caption{Visualy displays linear relationships}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/scatter-plot2.png}
    \caption{Scatter plot example with non-linear relationships}
\end{figure}
Other forms but refer to stat course:
\begin{itemize}
    \item QQ-Plot
    \item Scatter Plot Matrix
    \item Tree Maps
    \item Tag Clouds
\end{itemize}

\subsubsection{Missing Data}
\begin{itemize}
    \item Missing Completely at Random (MCAR); Sensor malfunction
    \item Missing at Random (MAR); Income missing by age
    \item Missing not at Random; Weight missing because of shame
\end{itemize}
Handling of missing data:
\begin{itemize}
    \item Discard samples (rows)
    \item Fill missing with estimation
    \begin{itemize}
        \item Central Tendencies (Mean, median, mode)
        \item  Global constant
        \item Regression
    \end{itemize}
    \item Maximum Likelihood
    \item ML Based Methods
\end{itemize}

\subsubsection{Nois}
Aproaches for noise:
\begin{itemize}
    \item Filtering
    \item Noise robust methods
    \item Regression, smoothing by data fitting
    \item Smoothing by bins
\end{itemize}

\subsubsection{Outliers}
Approaches for outliers:
\begin{itemize}
    \item Analyse
    \item Delete
    \item Binning
    \item Imputing
\end{itemize}

\subsubsection{Smoothing: Binning}
\begin{itemize}
    \item Equal bins (equi-depth)
    \item Bin Means
    \item Bin Boundaries
\end{itemize}

\subsubsection{Normalization - Scaling}
Normalization is essential to present data in a consistent state, enabling a fair comparison of the weight of 
different parameters. Larger numerical ranges for example can dominate an algorithm.
\begin{figure}[H]
    \begin{equation}
        v_i' = \frac{v_i}{10^j} \text{ where j is the smallest value such that } max(|v_j'|) < 1
    \end{equation}
    \caption{Decimal Scaling: E.g. take the biggest value of a set then take the "stellen" / digits for j.}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        v_i' = \frac{v_i - min_A}{max_A - min_A}
    \end{equation}
    \caption{Min-Max}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        \begin{split}
            \text{let } R = [min_R, max_R] \\
            v_i' = \frac{v_i - min_A}{max_A - min_A} \cdot (max_R - min_R) + min_R
        \end{split}
    \end{equation}
    \caption{Min-Max Range}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        \begin{split}
            v_i' = \frac{v_i - \mu}{\sigma} \\
            v_i' = \frac{v_i - \bar{A}}{S_A} \\
            \text{with few values or outliers: } S_A' = \frac{1}{N} (|v_1 - \bar{A}|) + \cdots
        \end{split}
    \end{equation}
    \caption{Z-Score}
\end{figure}

\subsubsection{Discretication}
Special form of data reduction this in turn:
\begin{itemize}
    \item Simplifies data
    \item More efficient mining
    \item Continous feature with smaller chance of correlation with target variable
    \item Easier to understand
    \item Many algorithms for nominal data
    \item Eliminate noise
\end{itemize}
Methods for discretication:
\begin{itemize}
    \item Bins of equal size
    \item Bins of equal intervals
    \item Clustering is a type of discretication
\end{itemize}
Binning boundaries must by disjoint and adjacent.

\subsubsection{Data Reduction}
\defn{Curse of Dimensionality (Again :X)}{
    Data is increasingly and exponentially sparce in the space it is in.
    Sparsity creates a difficult environment to achieve statistical significance.
    Definitions of norm (distance) and density become less useful.
}
Data reduction helps in:
\begin{itemize}
    \item Improve performance
    \item Visualize data for model selection
    \item Reduce dimensionality
    \item Remove noise
    \item Reducing complexity
    \item Increasing predictve accuracy
    \item Remove irrelevant data
\end{itemize}

Reduce dimensions for example by Sampling:
\begin{itemize}
    \item Simple Random Sample Without Replacement (SRSWOR)
    \item Simple Random Sample With Replacement (SRSWR)
    \item Stratified Sample
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/stratified-example.png}
    \caption{An example application of stratified sampling}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/feature-selection.png}
    \caption{Other methods for feature selection}
\end{figure}

\section{Association}
\subsection{Support}
Support reflects significance, hence the importance of an itemset.
With high support implying higher and low support lower relevance.
\begin{itemize}
    \item Occurence Frequency: Absolute number of transactions of that itemset (absolute support)
    \item Relative Support: Percentage of transactions
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/relative-frequency.png}
    \caption{Relative frequency as a measurement of support}
\end{figure}

\subsection{Confidence}
Is the support of a rule compared to the requirement of a rule.
It's a reliability measure, how certain, that B will occur given that A is present.
\begin{figure}[H]
    \begin{equation}
        Confidence(X \Rightarrow Y) = \frac{Support(U \cup Y)}{Support(X)} = P(Y|X)
    \end{equation}
\end{figure}
\subsection{Association Analysis Process}
It is called Apriori because it uses prior knowledge (i.e., the "A-priori property").
\begin{enumerate}
    \item How to identitfy subset of effective rules
    \item Data preparation
    \item Find frequent itemsets. Set up rules only for frequent itemsets
    \begin{itemize}
        \item Set minimum support \(\sigma\)
        \item Frequent 1-itemsets with support \(\geq \sigma\)
        \item Combine possible 1-itemsets
        \item Use frequent superset to generate next batch of candidates
    \end{itemize}
    \item Generate rules only from frequent itemsets and select only rules which are frequently true.
    \begin{itemize}
        \item Set minimum confidence \(\delta\)
        \item For each frequent itemset setup possible combinatorical rule
        \item Calculate confidence and select rules \(\geq \delta\)
    \end{itemize}
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/a-priori-association-example.png}
    \caption{A-Priori Association Analysis Example}
\end{figure}

Types of association include:
\begin{itemize}
    \item Boolean Association
    \item Quantitative Association
    \item Single and multidimensional Associations
    \item Abstraction Levels
\end{itemize}

\subsubsection{Lift Measure}
Is supplementary significance measure
used as correlation measure in market basket analysis.
Indicates overall significance, i.e. how much better does it predict the right hand side of the rule.
\begin{figure}[H]
    \begin{equation}
        Lift(X \Rightarrow Y) = \frac{Support(X \cup Y)}{Support(X) \cdot Support(Y)} = \frac{Confidence(X \Rightarrow Y)}{Support(Y)}
    \end{equation}
\end{figure}

\begin{lstlisting}[language=Python]
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules
fi_sets = apriori(mba, min_support=.05, use_colnames=True)
rules = association_rules(fi_sets, metric="confidence", min_threshold=.2)

\end{lstlisting}


\section{Classification}
\subsection{Metrics}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/cm.png}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        Precision = \frac{TP}{TP+FP}
    \end{equation}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        Recall = \frac{TP}{TP+FN}
    \end{equation}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        F = 2 * \frac{P \cdot R}{P + R} = \frac{2 TP}{2 TP + FN + FP}
    \end{equation}
\end{figure}
\subsection{OneR}
Is a simple rule-based method.
The name reflects the method's simplicity: it ultimately builds and uses only one rule based on one attribute.
We naively assume all attributes are independent of each other.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/oner-example.png}
\end{figure}
\subsection{Naive Bayes}
The Bayesian principle is to assign a case to the 
class that has the largest posterior probability.
The classifier is called naive because it does not respect additional rules like grammar and other considerations.
\begin{figure}[H]
    \begin{equation}
        P(A|B) = P(B|A) \frac{P(A)}{P(B)}
    \end{equation}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/naive-bayes.png}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        P(H|E) = P(Spam|Evidence) \Rightarrow P(Spam) P(Money | Spam) P(Hello | Spam) \cdots
    \end{equation}
    \caption{Naive Bayes example of a single hypothesis. We take the hypothesis with the highest likelihood.}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        P(H) = \frac{\text{number that belong to class }c_i}{\text{total number}}
    \end{equation}
    \caption{Prior; A-Priori}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        P(E|H) = \prod_i P(e_i|H)
    \end{equation}
    \caption{Since we naively assume attributes are independent we can multiply. \(e_i\) refers to an attribute
    that is present given that H is true. E.g. the word "money" is in an email that is indeed H=spam.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/naive-bayes-example.png}
\end{figure}

Optimizations:
\begin{itemize}
    \item Ignore predictor if every value is missing
    \item Ignore predictor if it has only one observed category
    \item Ignore case if value of the target variable is missing
    \item Ignore case if value of all predictors are missing
    \item Use only predictors for cases with nonmissing values
    \item Use binning for continuous variables
    \item Replace values by the parameters of a probability distribution
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/naive-bayes-pd-replacement.png}
    \caption{Example: Use parameters of a distribution to estimate the probability of numerical values}
\end{figure}

\begin{lstlisting}[language=Python]
# Reference: https://scikit-learn.org/stable/modules/naive_bayes.html
from sklearn.feature_extraction.text import TfidfVectorizer 
# TF_IDF alg assigns significance to every word within data
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

tfidf = TfidfVectorizer()

train_mod = tfidf.fit_transform(train.data)
model = MultinomialNB()

model.fit(train_mod, train.target)
test_mod = tfidf.transform(test.data)

pred = model.predict(test_mod)

accuracy_score(test.target, pred)
\end{lstlisting}

\begin{lstlisting}[language=Python]
from sklearn.metrics import confusion_matrix
# Comparing the expected categories with the predicted categories
matrix = confusion_matrix(test.target, pred)

fig, ax = plt.subplots(figsize=(10, 10))
sns.heatmap(
    matrix.T,
    square=True,
    annot=True,
    fmt="d",
    cbar=True,
    xticklabels=train.target_names,
    yticklabels=train.target_names,
    ax=ax
)
ax.set(xlabel="True Category", ylabel="Predicted Category")
\end{lstlisting}

\subsection{Decision Tree}
\subsubsection{ID3 Alhorithm}
Is the precurso to the C4.5 and is commonly used in ML and NLP.
It uses a greedy approach.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/id3-algorithm.png}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/tree-attribute-calc.png}
\end{figure}
Comparison with ID4.5:
\begin{itemize}
    \item Supports continous and discrete
    \item Handles incomplete data samples
    \item Solves overfitting by bottom up techniques
    \item Different weighting to features
\end{itemize}
\subsubsection{Pruning}
With the Gain Ratio metric one can punish a bigger count of nodes using split information.
\begin{figure}[H]
    \begin{equation}
        GR(Y,X) = \frac{I(Y,X)}{SI(Y,X)}
    \end{equation}
    \caption{Gain Ratio}
\end{figure}
\begin{figure}[H]
    \begin{equation}
        SI(Y,X) = - \sum_{m} \frac{|T_i|}{|T|} log_2 \frac{T_i}{T}
    \end{equation}
    \caption{Split Information}
\end{figure}
\subsubsection{Missing Values}
\begin{itemize}
    \item Replace missing attribute with mode
    \item Replace missing attribute X which has the highest count the target class 
\end{itemize}

\subsection{KNN}
KNN preserves the data, no model is created and calculations are done during classification.
The underlying idea is to classify by distance similarity using k-nearest samples.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/k-parameter-knn.png}
    \caption{The hyperparameter k dictates the amount of samples that dictate the class.}
\end{figure}


Prerequisites:
\begin{itemize}
    \item Enough examples
    \item Instances with no more than 20 attributes
    \item Ordinal or cardinal attributes mappable to \(\mathbb{R}^n\)
\end{itemize}

Pro:
\begin{itemize}
    \item Easy
    \item Fast training
    \item Training information not lost
    \item Allows regression of other classification tasks
\end{itemize}

Contra:
\begin{itemize}
    \item Possibly slow
    \item Memory based
    \item Can be fooled (irrelevant attributes)
\end{itemize}

\defn{Inverse Distance Weighting (IDW)}{
    Used to weight nearer neighbors to contribute more.
    Assigned valoues are calculated with weighted average.
    We weight the vote by the inverse of the square of the Euclidian distance.
}

\section{Ensemble}
In Ensewmble learning multiple models are combined to improve results.
Consult ML lecture for more information.
\begin{itemize}
    \item Bagging, \textbf{less variance}
    \item Boosting, \textbf{less bias, less variance}
    \item Stacking, \textbf{less bias, less variance}
\end{itemize}

\subsection{Bagging}
Initial idea is to parallelize learning by utilizing the same model multiple times.
For this we can use bootstrap aggregation by taking repeatad samples from the training set
with the underlying theory that this averaging will reduce variance.

\subsection{Random Forest}
Take bootstrapped samples (with replacement) and traing on bootstrapped set.
Classify by majority vote or average (soft vs hard-voting).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/random-forest.png}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/ensemble.png}
    \caption{This shows the effect of esemble methods. Compared to a single model predictions can have
    very hard edges. This may be because of the single model using a specific distance metric.
    Using multiple models we can now make this border more smooth, by e.g. averaging the predictions.}
\end{figure}


\subsection{Boosting}
Most known algorithm is AdaBoost. Boost increases accuracy and leads to
perfect training performance. On training data a specific count of model are created.
Its an iterative method, where we build models to correct the previous mistakes.
Each descendant model, improves the models which misclassify a lot.
It uses a weighted training set (starts with equal weighting).
Weights are adjusted after each model creation.
Wrongly classified samples get a bigger weight.
The result consists of multiple models where each model gets a different weight
depending on the achieved training performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/boosting.png}
\end{figure}

\subsection{Stacking}
In stacking multiple base learners provide predictions for the meta-learner.
Where the base-learner is also called Level-0-Modell and the metra learner
Level-1-Modell. One usually picks different base learner altorithms.

\section{Clustering}

\defn{General Clustering Process}{
    The basic goal is to partition a set of n objects into a set of k clusters.
    We use unsupervised learning to break up the set into a manageable number of classes.

    One can use different approaches:
    \begin{itemize}
        \item Partitioning Methods 
        \item Hierarchical Methods
        \item Model-based Methods
        \item Density-based Methods
        \item A Mix
    \end{itemize}

    Using this one can assess if elements are similar to on another.
}

\subsection{Common Distance Metrics}

Manhattan:
\begin{equation}
    d_{ij} = \sum |x_{ia} - x{ja}|
\end{equation}

Euclidean:
\begin{equation}
    d_{ij} = \sqrt{\sum |x_{ia} - x_{ja}|^2 }
\end{equation}

Minkowski:
\begin{equation}
    d_{ij} = [\sum |x_{ia}- x{ja}|^r]^{\frac{1}{r}} 
\end{equation}

Others include:
\begin{itemize}
    \item Pearson Correlation
    \item Canberra
    \item Mahalanobis
\end{itemize}

\subsection{K-Mean}
Partition N data points into K disjoint subsets.
Cluster center are not represented as a real instance but an average.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/k-means.png}
    \caption{When dim is greater or equal to 2 then its NP-Hard. 
    Finding the best solution in p-time is unfeasable.
    Iterative algorithms converges often quickly to local optimum.}
\end{figure}

K-means may work best for compact, distinct, sphere shaped clusters.
Results depend o0n initialization with hyperparameters like the cluster count.
Clusters are heavily influenced by outliers.

\subsection{K-Medoid}
Is a similar method to K-Means but uses a real instance as cluster center.
In Medoid one also uses the Manhattan distance.
Foreach non-center instance we calculate the total cost of swapping the center.
With the error after swap - Error before the swap.

We swap the current center for non-centers where the least total cost is \(< 0 \)

And as error:
\begin{equation}
    E = \sum_{j=1}^{k} \sum_{p \in C_j} |p - c_j|
\end{equation}

\subsection{Nominal Scales}

\defn{Hemming Distance}{
    Count the number of attributes at which the corresponding objects are different.
    Distance is increased by 1 if different.

    \begin{equation}
        D_H = \sum |x_i - y_i|
    \end{equation}

    With multiple attributes we scew into the range [0,1]:
    \begin{equation}
        d(x,y) = \frac{D_H(x,y)}{n}
    \end{equation}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/contingency-table.png}
    \caption{Distance Metrics for Non-Numerical Attributes. Showing binary distance using contingency table}
\end{figure}

\defn{Distance in Contingency Table}{
    \begin{equation}
        d(x_i, x_j) = \frac{b+c}{a+b+c+d}
    \end{equation}

    Jaccard Distance for asymmetric distributions (skewed):
    \begin{equation}
        d(x_i,x_j) = \frac{b+c}{a+b+c}
    \end{equation}
    We leave out negative matches.
}

\subsection{Ordinals}
\defn{Normalized Rank Transformation}{
    Convert the ordinal value into a rank by value \(r = [1,R]\).
    Normalize the rank into standardized values of zero to one.
    \begin{equation}
        r_N = \frac{r-1}{R-1}
    \end{equation}
    Finally calculate the distance.
}

\subsection{Hierarchical Clustering}

In HC we start by treating each sample as cluster.
Then we iteratively identify the two closest clusters and merge them.
The key operation is the proximity computiation also called the \textbf{linkage criteria}.

Types:
\begin{itemize}
    \item Bottom-Up
    \item Top-Down, Start with a single cluster then split
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/hierarchical-clustering.png}
\end{figure}

Linkage Criteria / Fusion Method
\begin{itemize}
    \item Single Linkage, Smallest distance between two clusters, tends to chaining, cannot separate if there is noise between clusters ,can separate non-elliptical shapes
    \item Complete Linkage, Take the similarity of the farthest points within two clusters, works well for noise, avoids chaining, suffers from crowding, tends to break large cluster
    \item Average Linkage, 
    \item Centroid
    \item Ward, sum of the square of the distances, minimize the increase of withing cluster variance
    \begin{itemize}
        \item \begin{equation*}
            W(C) = \sum |x_i - \mu_C |^2
        \end{equation*}
        \item \begin{equation*}
            \Delta W = W(C) - (W(A) + W(B))
        \end{equation*}
    \end{itemize}
\end{itemize}

\subsection{Validating Clusters}

\begin{itemize}
    \item Internal Criteria, often compactness, connectedness and separation
    \item External Criteria, comparison with external result
    \item Relative Criteria, results with altered parameters or other algorithms
\end{itemize}

\defn{Compactness}{
    Describes:
    \begin{itemize}
        \item Instances in a cluster are close
        \item Intraclass similarity is high
        \item Distances within a cluster
    \end{itemize}
}

\defn{Cluster density}{
    \begin{eqnarray}
        den(C) = \frac{1}{n} \sum (x_i - c)
    \end{eqnarray}
}

\defn{Cluster Variance}{
    \begin{equation}
        var(C) = \sum (x_i - C)^2
    \end{equation}
    And average variance:
    \begin{equation}
        var(C) = \frac{1}{n} \sum (x_i - C)^2
    \end{equation}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{Images/datan/cluster-validation.png}
    \caption{Compactness for validating cluster: The typical knee-shape is visible, where at a specific
    amount of clusters the density does not decrease by much.}
\end{figure}

\defn{Seperation}{
    Describes the distance between clusters.
    \begin{itemize}
        \item Single Linkage
        \item Complete Linkage
        \item Average Linkage
        \item Distance of Cluster Centers
    \end{itemize}
}

\end{document}
